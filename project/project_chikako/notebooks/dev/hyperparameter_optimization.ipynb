{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<module 'defs' from '/home/jovyan/dev/projects/vinx/defs.py'>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 12
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# coding=utf-8#%%\n",
    "\n",
    "import defs as d\n",
    "import os\n",
    "import pandas as pd\n",
    "# Use the LightGBM Tuner\n",
    "# https://medium.com/optuna/lightgbm-tuner-new-optuna-integration-for-hyperparameter-optimization-8b7095e99258\n",
    "# import optuna.integration.lightgbm as lgb_tuner\n",
    "import lightgbm as lgb\n",
    "import loss_functions as lf\n",
    "import base_datasource as bd\n",
    "import time_series_cross_validation as cv\n",
    "import json\n",
    "import optuna\n",
    "import numpy as np\n",
    "import parallel as pl\n",
    "from joblib import delayed\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "start\n",
      "CPU times: user 75.8 ms, sys: 194 ms, total: 270 ms\n",
      "Wall time: 535 ms\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "(210958, 50)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 13
    }
   ],
   "source": [
    "%%time\n",
    "print('start')\n",
    "data = pd.read_pickle(d.TMP+'/merged_data.pkl')\n",
    "data.shape\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "start\n",
      "CPU times: user 785 Âµs, sys: 1.03 ms, total: 1.82 ms\n",
      "Wall time: 1.76 ms\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "%%time\n",
    "print('start')\n",
    "def objective(trial):\n",
    "  params = {\n",
    "    'objective': 'gamma',\n",
    "    'metric': 'fair',\n",
    "    'verbosity': -1,\n",
    "    'boosting_type': 'gbdt',\n",
    "\n",
    "    # Bagging will randomly select part of data without resampling.\n",
    "    # With time-series nature of our problem, bagging should be disabled.\n",
    "    'bagging_fraction': 1,\n",
    "    'bagging_freq': 0,\n",
    "\n",
    "    # This parameter should not be zero (meaning no limit).\n",
    "    # Need to find the best interval.\n",
    "    'max_depth': 0,\n",
    "\n",
    "    # Shrinkage rate\n",
    "    'learning_rate': trial.suggest_uniform('learning_rate', 0.05, 0.2),\n",
    "\n",
    "    # Take boosting_round large and early-stop if the metric doesn't improve.\n",
    "    # 'num_boost_round': trial.suggest_int('num_boost_round', 100, 200),\n",
    "    'num_boost_round': 400,\n",
    "    'early_stopping': 10,\n",
    "\n",
    "    # Each value of `store_cd` should have its own branch.\n",
    "    # If you accept this claim, there should at least be 185 leaves(region).\n",
    "    # If you split 2 region for each feature, then there will be 2^num_features regions(leaves).\n",
    "    'num_leaves': trial.suggest_int('num_leaves', 400, 600),\n",
    "\n",
    "    # Regularization parameters.\n",
    "    'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n",
    "    'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n",
    "\n",
    "    # Fraction of features that lgbm uses for each training.\n",
    "    # Lower value means faster training.\n",
    "    # seed value is the default = 2\n",
    "    'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 0.8),\n",
    "    'feature_fraction_seed': 2,\n",
    "\n",
    "    # This parameter is for ensuring LLN should not be randomly tuned.\n",
    "    'min_data_in_leaf': trial.suggest_categorical('min_data_in_leaf', [20, 50, 100]),\n",
    "  }\n",
    "\n",
    "  estimator = lgb.LGBMRegressor()\n",
    "  metric = lf.mape\n",
    "  scores = cv.cross_val_score(\n",
    "    estimator\n",
    "    ,params\n",
    "    ,metric\n",
    "    ,data\n",
    "    ,lgbm_tuner=False\n",
    "    ,random_seed=1\n",
    "    ,n_jobs=4\n",
    "    ,verbosity=50\n",
    "  )\n",
    "\n",
    "  # Store results in a external storage\n",
    "  scores_df = pd.DataFrame(scores)\n",
    "  mask = (scores_df['score'] >= 0)\n",
    "  scores_df = scores_df[mask].copy()\n",
    "  scores_df['trial_number'] = 10\n",
    "  scores_df['trial_params'] = json.dumps(trial.params)\n",
    "  scores_df['mean_score'] = scores_df['score'].mean()\n",
    "  if trial.number == 0:\n",
    "    cmd = \"echo 'trial_number, split_date, score, mean_socre, params' > \"+d.TMP+'/trial.csv'\n",
    "    os.system(cmd)\n",
    "    scores_df.to_csv(d.TMP+'/trial.csv', index=False)\n",
    "  else:\n",
    "    old_scores_df = pd.read_csv(d.TMP+'/trial.csv')\n",
    "    merged_scores = pd.concat([old_scores_df, scores_df], axis=0, ignore_index=True)\n",
    "    merged_scores.to_csv(d.TMP+'/trial.csv', index=False)\n",
    "\n",
    "  return scores_df['score'].mean()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "start\n",
      "valid_y.shape:  5328\n",
      "Non-Zero cnt:  5328\n",
      "Score: 0.04627608714525071\n",
      "valid_y.shape:  5425\n",
      "Non-Zero cnt:  5425\n",
      "Score: 0.0383842774009598\n",
      "valid_y.shape:  5688\n",
      "Non-Zero cnt:  5688\n",
      "Score: 0.0916744896401746\n",
      "valid_y.shape:  5735\n",
      "Non-Zero cnt:  5735\n",
      "Score: 0.04199747153639968\n",
      "valid_y.shape:  5735\n",
      "Non-Zero cnt:  5735\n",
      "Score: 0.041567232976027595\n",
      "valid_y.shape:  5766\n",
      "Non-Zero cnt:  5766\n",
      "Score: 0.06363607604359114\n",
      "valid_y.shape:  5766\n",
      "Non-Zero cnt:  5766\n",
      "Score: 0.05423668216311856\n",
      "valid_y.shape:  5762\n",
      "Non-Zero cnt:  5762\n",
      "Score: 0.061614965632212554\n",
      "valid_y.shape:  5790\n",
      "Non-Zero cnt:  5790\n",
      "Score: 0.08358664632697484\n",
      "valid_y.shape:  5795\n",
      "Non-Zero cnt:  5795\n",
      "Score: 0.09622933533641168\n",
      "valid_y.shape:  5797\n",
      "Non-Zero cnt:  5797\n",
      "Score: 0.041299780668369916\n",
      "valid_y.shape:  5904\n",
      "Non-Zero cnt:  5809\n",
      "end\n",
      "CPU times: user 17min 8s, sys: 25.6 s, total: 17min 34s\n",
      "Wall time: 2min 15s\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": [
      "/opt/conda/envs/azureml/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning:\n",
      "\n",
      "Found `num_boost_round` in params. Will use it instead of argument\n",
      "\n",
      "/opt/conda/envs/azureml/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning:\n",
      "\n",
      "Found `early_stopping` in params. Will use it instead of argument\n",
      "\n",
      "/opt/conda/envs/azureml/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning:\n",
      "\n",
      "Found `num_boost_round` in params. Will use it instead of argument\n",
      "\n",
      "/opt/conda/envs/azureml/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning:\n",
      "\n",
      "Found `early_stopping` in params. Will use it instead of argument\n",
      "\n",
      "/opt/conda/envs/azureml/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning:\n",
      "\n",
      "Found `num_boost_round` in params. Will use it instead of argument\n",
      "\n",
      "/opt/conda/envs/azureml/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning:\n",
      "\n",
      "Found `early_stopping` in params. Will use it instead of argument\n",
      "\n",
      "/opt/conda/envs/azureml/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning:\n",
      "\n",
      "Found `num_boost_round` in params. Will use it instead of argument\n",
      "\n",
      "/opt/conda/envs/azureml/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning:\n",
      "\n",
      "Found `early_stopping` in params. Will use it instead of argument\n",
      "\n",
      "/opt/conda/envs/azureml/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning:\n",
      "\n",
      "Found `num_boost_round` in params. Will use it instead of argument\n",
      "\n",
      "/opt/conda/envs/azureml/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning:\n",
      "\n",
      "Found `early_stopping` in params. Will use it instead of argument\n",
      "\n",
      "/opt/conda/envs/azureml/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning:\n",
      "\n",
      "Found `num_boost_round` in params. Will use it instead of argument\n",
      "\n",
      "/opt/conda/envs/azureml/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning:\n",
      "\n",
      "Found `early_stopping` in params. Will use it instead of argument\n",
      "\n",
      "/opt/conda/envs/azureml/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning:\n",
      "\n",
      "Found `num_boost_round` in params. Will use it instead of argument\n",
      "\n",
      "/opt/conda/envs/azureml/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning:\n",
      "\n",
      "Found `early_stopping` in params. Will use it instead of argument\n",
      "\n",
      "/opt/conda/envs/azureml/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning:\n",
      "\n",
      "Found `num_boost_round` in params. Will use it instead of argument\n",
      "\n",
      "/opt/conda/envs/azureml/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning:\n",
      "\n",
      "Found `early_stopping` in params. Will use it instead of argument\n",
      "\n",
      "/opt/conda/envs/azureml/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning:\n",
      "\n",
      "Found `num_boost_round` in params. Will use it instead of argument\n",
      "\n",
      "/opt/conda/envs/azureml/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning:\n",
      "\n",
      "Found `early_stopping` in params. Will use it instead of argument\n",
      "\n",
      "/opt/conda/envs/azureml/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning:\n",
      "\n",
      "Found `num_boost_round` in params. Will use it instead of argument\n",
      "\n",
      "/opt/conda/envs/azureml/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning:\n",
      "\n",
      "Found `early_stopping` in params. Will use it instead of argument\n",
      "\n",
      "/opt/conda/envs/azureml/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning:\n",
      "\n",
      "Found `num_boost_round` in params. Will use it instead of argument\n",
      "\n",
      "/opt/conda/envs/azureml/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning:\n",
      "\n",
      "Found `early_stopping` in params. Will use it instead of argument\n",
      "\n",
      "[I 2020-07-04 12:22:14,970] Finished trial#0 with value: 0.060045731351771915 with parameters: {'learning_rate': 0.1543194350518498, 'num_leaves': 520, 'lambda_l1': 0.2971416053315697, 'lambda_l2': 0.020970380921065086, 'feature_fraction': 0.5972032021074605, 'min_data_in_leaf': 100}. Best is trial#0 with value: 0.060045731351771915.\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 3h 9min 51s\n",
    "# 5h 9min 21s\n",
    "print('start')\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=1, n_jobs=1)\n",
    "print('end')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "CPU times: user 3.58 ms, sys: 6 Âµs, total: 3.59 ms\n",
      "Wall time: 2.94 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": "'{\\n  \"learning_rate\": 0.1543194350518498,\\n  \"num_leaves\": 520,\\n  \"lambda_l1\": 0.2971416053315697,\\n  \"lambda_l2\": 0.020970380921065086,\\n  \"feature_fraction\": 0.5972032021074605,\\n  \"min_data_in_leaf\": 100\\n}'"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "print('start')\n",
    "# pd.DataFrame(study.best_params)\n",
    "json.dumps(study.best_params, indent=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "CPU times: user 30.7 ms, sys: 0 ns, total: 30.7 ms\n",
      "Wall time: 42.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('start')\n",
    "df = study.trials_dataframe()\n",
    "df.to_csv(d.TMP+'/trial_100_sampling_122_horizon_30_lgbm.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}